# Project_7_ML

# Introducci√≥n

Este proyecto tiene como objetivo abordar una pregunta clave para viajeros y anfitriones por igual: ¬øcu√°nto deber√≠a costar una estancia en una ubicaci√≥n espec√≠fica en los EE. UU.? Al aprovechar t√©cnicas avanzadas de an√°lisis de datos y machine learning, nos sumergiremos en un oc√©ano de variables, desde la ubicaci√≥n geogr√°fica y las comodidades del alojamiento hasta las tendencias del mercado y eventos locales.

La capacidad de predecir con precisi√≥n los precios no solo beneficia a los viajeros al planificar su presupuesto, sino que tambi√©n brinda a los anfitriones una visi√≥n estrat√©gica para establecer tarifas competitivas y atractivas. Este proyecto no solo es una exploraci√≥n de datos, sino una puerta abierta a la optimizaci√≥n de experiencias de viaje y a un mercado de alojamientos m√°s transparente y eficiente.

Acomp√°√±anos en este viaje anal√≠tico mientras desentra√±amos patrones, identificamos factores clave y creamos un modelo predictivo robusto que ilumine el fascinante universo de los precios de Airbnb en los Estados Unidos. ¬°Bienvenidos a un futuro donde cada viaje es una experiencia a√∫n m√°s personalizada y accesible! üåçüè°‚ú®

# Data Cleansing

1. Examinamos los dataframes de train y test
2. Buscamos una alta correlaci√≥n entre las variables num√©ricas para evitar problemas de multicolinealidad y overfitting, pero no se aprecian en el heatmap. S√≥lo entre la "latitud" y la "longitud", pero ambas ser√°n necesarias y complementarias.
3. Covertimos a valores num√©ricos todas las columnas de valor para nuestro modelo predictivo con la ayuda de LabelEncoder y One Hot Encoding. Desechamos las que no aporten nada.
4. Tratamos de clusterizar en agrupaciones de 10 valores distintos los datos de las columnas num√©ricas, sirviendo esto para reducir el ruido centr√°ndonos en las caracter√≠sticas principales y simplificar el modelo.
5. Descomponemos la columna "amenities" en otras nuevas en proporci√≥n a su numero y las integramos a los dataframes para facilitar su posterior interpretaci√≥n.
6. Aunque carecemos de un gran n√∫mero de missing values, los rellenaremos con la moda de los valores de sus respectivas series.
7. Para finalizar el proceso de limpieza de datos, daremos el mismo orden y n√∫mero de columnas al train y al test, necesario para un correcto entrenamiento de los datos.

# Entrenamiento y predicci√≥n

- Si tus datos tienen patrones complejos o relaciones no lineales, Random Forest puede ser m√°s adecuado para capturar esas complejidades.
- Decision Tree Regressor puede ser propenso al sobreajuste (overfitting) si no se controla adecuadamente. Random Forest mitiga este problema al promediar las predicciones de varios √°rboles, lo que puede resultar en un modelo m√°s robusto y generalizable.
- Random Forest tiene la capacidad inherente de manejar caracter√≠sticas relevantes al construir √°rboles con subconjuntos aleatorios de caracter√≠sticas. Esto puede ser beneficioso cuando tienes un conjunto de datos con muchas caracter√≠sticas, algunas de las cuales pueden no ser informativas para la tarea de regresi√≥n.
- Decision Tree y Random Forest son algoritmos m√°s robustos frente a outliers en comparaci√≥n con Linear Regression. Los √°rboles de decisi√≥n no se ven tan afectados por valores at√≠picos y pueden adaptarse mejor a la variabilidad en los datos.
- Random Forest y Decision Tree Regressor no hacen suposiciones estrictas sobre la distribuci√≥n de los datos, a diferencia de Linear Regression, que asume una relaci√≥n lineal entre las variables independientes y dependientes.
- Random Forest tiende a ser m√°s f√°cil de usar y generalmente requiere menos ajuste de hiperpar√°metros en comparaci√≥n con Decision Tree Regressor. Esto puede simplificar el proceso de modelado y aumentar la probabilidad de obtener buenos resultados sin mucha afinaci√≥n.

# Observaciones

- Decidimos no normalizar el dato con Standard Scaler o MinMaxScaler porque no daba los resultados esperados, empeorando el modelo.
- Conseguimos mejorar el resultado a posterioiri eliminando la columna "id". A pesar de ser num√©rica, ten√≠a un efecto categ√≥rico que empobrec√≠a la predicci√≥n,
